{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: Neural Nets, a practical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a general OO implementation of feed forward neural networks. As of now, it supports fully connected layers with different activation functions (i.e., logits, ReLU, and linear). It also supports the softmax operation for calssification purposes. Two loss functions are currently implemented:\n",
    "- MSE\n",
    "- Cross Entropy with logits (Multiclass)\n",
    "\n",
    "Backpropagation is implemented using automatic differentiation. This is achieved by registering in each leayer the corresponding derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized ReLU and the corresponding gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def ReLU_d(x):\n",
    "    return np.array(x > 0, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized logits and the corresponding gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logits(x):\n",
    "    return  1 / (1 + np.exp(-x))\n",
    "\n",
    "def logits_d(x):\n",
    "    return np.exp(x) / np.square((1 + np.exp(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized logits and the corresponding gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_d(x):\n",
    "    return np.ones(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Cost, and corresponding error in the last layer (delta_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(predictions, targets):\n",
    "    return np.square(predictions-targets)\n",
    "\n",
    "def MSE_dL(predictions, targets, z=None):\n",
    "    return -2 * (targets - predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy, and corresponding error in the last layer (delta_L)\n",
    "Refer to the lecture notes to see how delta_L is computed for cross entropy loss with logits and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets):\n",
    "    predictions = softmax(predictions)\n",
    "    return -np.sum(targets * np.log(predictions))\n",
    "\n",
    "def cross_entropy_dL_with_logits(predictions, targets, z):\n",
    "    classes = len(targets)\n",
    "    error = np.zeros(classes)\n",
    "    norm_factor = np.sum(np.exp(predictions))\n",
    "    for j in range(classes):\n",
    "        tmp = 0\n",
    "        for k in range(classes):\n",
    "            if k == j:\n",
    "                tmp -= targets[k] * (1 - np.exp(predictions[k]) / norm_factor)\n",
    "            else:\n",
    "                tmp -= targets[k] * (-np.exp(predictions[k]) / norm_factor)\n",
    "        tmp *= logits(z[j]) * (1 - logits(z[j]))\n",
    "        error[j] = tmp\n",
    "    return error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class implementing a fully connected layer. \n",
    "This implementation is agnostic of the specific activation function, and uses the concept of higher order functions to provide a general framework to define layers. Note that in this way you can implement new activation functions without having to change the class implementing the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, input_size, layer_size, activation=linear, activation_d=linear_d):\n",
    "        \n",
    "        # here the constructor takes as parameters the activation function and the corresponding gradient,\n",
    "        # this keeps the implementation general, and, in turn, allows to implement a generic version of\n",
    "        # the backpropagation algorithm\n",
    "        \n",
    "        self.activation_function = activation\n",
    "        self.activation_function_d = activation_d\n",
    "        \n",
    "        # Size of the weight matrix and bias vector\n",
    "        self.input_size = input_size # K\n",
    "        self.layer_size = layer_size # J\n",
    "        \n",
    "        # Weigth Matrix has input_size columns and layer_size rows (J x K)\n",
    "        # I initialize the weights using He initialization\n",
    "        self.W = np.random.randn(self.layer_size, self.input_size) * np.sqrt(2 / self.input_size)\n",
    "        \n",
    "        # Bias Vector, I initialize it to zero\n",
    "        self.b = np.zeros(self.layer_size)\n",
    "        \n",
    "        # Input to activation function, I initialize it to zero\n",
    "        self.z = np.zeros(self.layer_size)\n",
    "        \n",
    "        # Activations \n",
    "        self.a = np.zeros(self.layer_size)\n",
    "        \n",
    "        # Gradients\n",
    "        self.W_grad = np.zeros((self.layer_size, self.input_size))\n",
    "        self.b_grad = np.zeros(self.layer_size)\n",
    "    \n",
    "    # reset the gradients to zero (to be called between batches)\n",
    "    def reset(self):\n",
    "        self.W_grad = np.zeros((self.layer_size, self.input_size))\n",
    "        self.b_grad = np.zeros(self.layer_size)\n",
    "    \n",
    "    # perform a forward pass\n",
    "    def forward(self, x):\n",
    "        self.z = np.matmul(self.W, x) + self.b\n",
    "        self.a = self.activation_function(self.z)\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class implementing a trainable feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, trainFlag):\n",
    "        self.trainFlag = trainFlag\n",
    "        self.layers = []\n",
    "        self.deltas = []\n",
    "        self.input = None\n",
    "    \n",
    "    # to be called to define\n",
    "    def add_layer(self, l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        self.deltas = []\n",
    "        a = x.copy()\n",
    "        self.input = a\n",
    "        for layer_dict in self.layers:\n",
    "            layer = layer_dict['layer']\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    # Reset the gradients between mini batches\n",
    "    def reset(self):\n",
    "        for layer_dict in self.layers:\n",
    "            layer_dict['layer'].reset()\n",
    "    \n",
    "    def aggregate(self, batch_size):\n",
    "        for layer_dict in self.layers:\n",
    "            layer = layer_dict['layer']\n",
    "            layer.W_grad /= batch_size\n",
    "            layer.b_grad /= batch_size\n",
    "    \n",
    "    # compute all the gradients of the cost function\n",
    "    def backward_pass(self, deltaL):\n",
    "        self.deltas = [deltaL]\n",
    "        size = len(self.layers) - 1\n",
    "        for i,j in zip(reversed(range(size)), [i for i in range(size)]):\n",
    "            W_lp1 = self.layers[i+1]['layer'].W\n",
    "            d_lp1 = self.deltas[j]\n",
    "            z_l = self.layers[i]['layer'].z\n",
    "            derivative = self.layers[i]['layer'].activation_function_d\n",
    "            \n",
    "            d_l = np.matmul(d_lp1, W_lp1) * derivative(z_l)\n",
    "            self.deltas.append(d_l)\n",
    "            \n",
    "        for layer_idx, i in zip(range(len(self.layers)), reversed(range(len(self.deltas)))):\n",
    "            prev_activations = self.layers[layer_idx - 1]['layer'].a if layer_idx > 0 else self.input\n",
    "            layer = self.layers[layer_idx]['layer']\n",
    "            layer.b_grad += self.deltas[i]\n",
    "            layer.W_grad += np.array([prev_activations * d for d in self.deltas[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(model, batch_size, learning_rate):\n",
    "    model.aggregate(batch_size)\n",
    "    layers = model.layers\n",
    "    for layer_dict in layers:\n",
    "        layer = layer_dict['layer']\n",
    "        layer.W -= learning_rate * layer.W_grad\n",
    "        layer.b -= learning_rate * layer.b_grad\n",
    "    model.reset()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = utils.load_data_class(fname='dataset.csv')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "NORM_FACTOR = 1000\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "loss = []\n",
    "accuracy_train = []\n",
    "accuracy_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** ACCURACY ON TEST 49.586% [untrained network]\n"
     ]
    }
   ],
   "source": [
    "net = Network(True)\n",
    "l_1 = {\n",
    "    'layer': Layer(2, 64, ReLU, ReLU_d),\n",
    "    'input_size': 2,\n",
    "    'layer_size': 64,\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "l_2 = {\n",
    "    'layer': Layer(64, 64, ReLU, ReLU_d),\n",
    "    'input_size': 64,\n",
    "    'layer_size': 64,\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "l_3 = {\n",
    "    'layer': Layer(64, 2, logits, logits_d),\n",
    "    'input_size': 64,\n",
    "    'layer_size': 2,\n",
    "    'activation': 'logits'\n",
    "}\n",
    "\n",
    "net.add_layer(l_1)\n",
    "net.add_layer(l_2)\n",
    "net.add_layer(l_3)\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "for sample, target in zip(test_X, test_y):\n",
    "        prediction = (np.argmax(softmax(net.forward_pass(sample))))\n",
    "        true_class = int(np.argmax(target))\n",
    "        accuracy += (prediction == true_class)\n",
    "\n",
    "accuracy /= test_X.shape[0]\n",
    "\n",
    "print('*' * 10 + ' ACCURACY ON TEST {:.3f}% [untrained network]'.format(accuracy * 100))\n",
    "accuracy_test.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop, and accuracy checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE EPOCH 1, LOSS 0.679\n",
      "********** ACCURACY ON TEST 49.633%\n",
      "DONE EPOCH 2, LOSS 0.633\n",
      "********** ACCURACY ON TEST 73.227%\n",
      "DONE EPOCH 3, LOSS 0.588\n",
      "********** ACCURACY ON TEST 94.742%\n",
      "DONE EPOCH 4, LOSS 0.558\n",
      "********** ACCURACY ON TEST 99.219%\n",
      "DONE EPOCH 5, LOSS 0.534\n",
      "********** ACCURACY ON TEST 98.727%\n",
      "DONE EPOCH 6, LOSS 0.513\n",
      "********** ACCURACY ON TEST 99.000%\n",
      "DONE EPOCH 7, LOSS 0.495\n",
      "********** ACCURACY ON TEST 99.320%\n",
      "DONE EPOCH 8, LOSS 0.480\n",
      "********** ACCURACY ON TEST 99.594%\n",
      "DONE EPOCH 9, LOSS 0.467\n",
      "********** ACCURACY ON TEST 99.578%\n",
      "DONE EPOCH 10, LOSS 0.457\n",
      "********** ACCURACY ON TEST 99.555%\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    for i in range(train_X.shape[0] // BATCH_SIZE):\n",
    "        batch_samples = train_X[i * BATCH_SIZE:(i+1) * BATCH_SIZE]\n",
    "        batch_targets = train_y[i * BATCH_SIZE:(i+1) * BATCH_SIZE]\n",
    "        tmp_loss = []\n",
    "        for sample, target in zip(batch_samples, batch_targets):\n",
    "            prediction = net.forward_pass(sample / NORM_FACTOR)\n",
    "            tmp_loss.append(cross_entropy(prediction, target))\n",
    "            deltaL = cross_entropy_dL_with_logits(predictions=prediction, \n",
    "                                                  targets=target, \n",
    "                                                  z=net.layers[-1]['layer'].z)\n",
    "            net.backward_pass(deltaL=deltaL)\n",
    "            \n",
    "        loss.append(np.mean(tmp_loss))\n",
    "        \n",
    "        net.aggregate(BATCH_SIZE)\n",
    "        \n",
    "        optimize(model=net, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 learning_rate=LEARNING_RATE)\n",
    "        #print(net.layers[0]['layer'].W_grad)\n",
    "        \n",
    "    print('DONE EPOCH {}, LOSS {:.3f}'.format(e + 1, loss[-1]))\n",
    "    \n",
    "    accuracy = 0\n",
    "    for sample, target in zip(test_X, test_y):\n",
    "        prediction = (np.argmax(softmax(net.forward_pass(sample / NORM_FACTOR))))\n",
    "        true_class = int(np.argmax(target))\n",
    "        accuracy += (prediction == true_class)\n",
    "\n",
    "    accuracy /= test_X.shape[0]\n",
    "\n",
    "    print('*' * 10 + ' ACCURACY ON TEST {:.3f}%'.format(accuracy * 100))\n",
    "    accuracy_test.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
